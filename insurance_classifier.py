import openai
import os
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer, util
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
import torch
import time

# ðŸ”¹ 1. ConfigurÄƒm clientul OpenAI cu API Key din variabila de mediu (fÄƒrÄƒ hardcoding)
client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ðŸ”¹ 2. ÃŽncÄƒrcÄƒm datele
company_df = pd.read_csv("ml_insurance_challenge.csv")  # Datele companiilor
taxonomy_df = pd.read_excel("insurance_taxonomy.xlsx", engine="openpyxl")  # Taxonomia

# ðŸ”¹ 3. PregÄƒtim textul pentru clasificare
company_df["full_description"] = company_df[["business_tags", "sector", "category", "niche"]].apply(
    lambda row: " ".join(filter(None, map(str, row))), axis=1
).fillna("")

# ðŸ”¹ 4. Model NLP: Sentence-BERT pentru embeddings
model = SentenceTransformer("all-MiniLM-L6-v2")

# ðŸ”¹ 5. CreÄƒm embeddings pentru companii È™i etichetele taxonomiei
company_embeddings = model.encode(company_df["full_description"], convert_to_tensor=True)
taxonomy_embeddings = model.encode(taxonomy_df["label"], convert_to_tensor=True)

# ðŸ”¹ 6. CalculÄƒm similaritatea cosinus dintre companii È™i etichete
sbert_sim = util.pytorch_cos_sim(company_embeddings, taxonomy_embeddings).cpu().numpy()

# ðŸ”¹ 7. TF-IDF pentru potrivirea bazatÄƒ pe cuvinte cheie
vectorizer = TfidfVectorizer(stop_words="english")
company_vectors = vectorizer.fit_transform(company_df["full_description"])
taxonomy_vectors = vectorizer.transform(taxonomy_df["label"])
tfidf_sim = cosine_similarity(company_vectors, taxonomy_vectors)

# ðŸ”¹ 8. Fusion Strategy: combinÄƒm scorurile SBERT È™i TF-IDF
final_sim = (sbert_sim * 0.7) + (tfidf_sim * 0.3)  # SBERT = 70%, TF-IDF = 30%

# ðŸ”¹ 9. Alegem cele mai relevante etichete pentru fiecare companie
top_n = 3  # Alegem cele mai bune 3 etichete
threshold = 0.2  # EliminÄƒm etichetele cu scoruri foarte mici

matched_labels = []
for row in final_sim:
    top_indices = np.argsort(-row)[:top_n]  # SortÄƒm descrescÄƒtor
    selected_labels = [taxonomy_df["label"].iloc[i] for i in top_indices if row[i] > threshold]
    matched_labels.append(selected_labels if selected_labels else ["Unclassified"])

# ðŸ”¹ 10. Folosim un model de la Hugging Face pentru clasificare
# Folosim `distilbert-base-uncased`, care este un model performant È™i accesibil
model_name = "distilbert-base-uncased"  # Modelul pre-antrenat pentru clasificare

# ðŸ”¹ 11. ÃŽncarcÄƒ tokenizer È™i model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# ðŸ”¹ 12. VerificÄƒm dispozitivul disponibil (GPU sau CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# CreÄƒm pipeline-ul pentru clasificare text
classifier = pipeline("text-classification", model=model, tokenizer=tokenizer, device=device.index if device.type == "cuda" else -1)

def classify_with_huggingface(description, candidate_labels):
    """ UtilizeazÄƒ modelul Hugging Face pentru a rafina clasificarea """
    prompt = f"""
    You are an expert in business classification for the insurance industry.
    Given the company description: "{description}", and the following candidate labels: {candidate_labels},
    return the most accurate labels from the given list.
    """
    
    # VerificÄƒm dacÄƒ candidate_labels nu este gol
    if not candidate_labels:
        print(f"Warning: No candidate labels for description: {description[:30]}...")
        return "Unclassified"  # ReturneazÄƒ o etichetÄƒ implicitÄƒ
    
    # UtilizÄƒm modelul pentru clasificare
    try:
        # TokenizÄƒm descrierea
        inputs = tokenizer(description, return_tensors="pt", truncation=True, padding=True).to(device)
        outputs = model(**inputs)
        
        # Alegem eticheta cu probabilitatea cea mai mare
        predicted_label_idx = outputs.logits.argmax(dim=-1).item()

        # DacÄƒ indexul este Ã®n afacerea limitei candidate_labels
        if predicted_label_idx >= len(candidate_labels):
            print(f"Warning: Predicted index {predicted_label_idx} is out of range for candidate labels.")
            predicted_label = candidate_labels[0]  # ReturnÄƒm prima etichetÄƒ ca fallback
        else:
            predicted_label = candidate_labels[predicted_label_idx]
        
        # AfiÈ™Äƒm eticheta aleasÄƒ pentru fiecare companie
        print(f"Selected label for description '{description[:50]}...': {predicted_label}")
        
        return predicted_label
    except Exception as e:
        print(f"Error with Hugging Face: {e}")
        return ", ".join(candidate_labels)

# ðŸ”¹ 13. AplicÄƒm Hugging Face pentru fiecare companie
final_labels = []
for i, row in company_df.iterrows():
    print(f"Processing company {i+1}/{len(company_df)}...")
    
    # TentÄƒm sÄƒ clasificÄƒm folosind Hugging Face
    refined_labels = classify_with_huggingface(row["full_description"], matched_labels[i])
    
    # DacÄƒ "Unclassified", folosim etichetele cele mai bune din similitudinea SBERT È™i TF-IDF
    if refined_labels == "Unclassified":
        print(f"Warning: Company {i+1} is unclassified. Using fallback based on SBERT & TF-IDF scores.")
        # Alegem etichetele cele mai relevante bazate pe scorurile SBERT È™i TF-IDF
        top_indices = np.argsort(-final_sim[i])[:top_n]
        refined_labels = [taxonomy_df["label"].iloc[i] for i in top_indices if final_sim[i][i] > threshold]
        refined_labels = refined_labels[0] if refined_labels else "Unclassified"
        
    final_labels.append(refined_labels)
    time.sleep(0.02)  # RespectÄƒm rate limits pentru API

company_df["insurance_label"] = final_labels

# ðŸ”¹ 14. SalvÄƒm rezultatele
company_df.to_csv("classified_companies_huggingface.csv", index=False)

print("âœ… Clasificarea cu Hugging Face a fost finalizatÄƒ! Rezultatele sunt Ã®n 'classified_companies_huggingface.csv'.")
